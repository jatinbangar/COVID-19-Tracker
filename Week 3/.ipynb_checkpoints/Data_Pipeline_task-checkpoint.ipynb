{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updates to our Spark-Job.py script\n",
    "* The path to our S3 data will now be passed as arguments instead of hardcoded values\n",
    "<code>\n",
    "phi_path = sys.argv[1]\n",
    "john_hopkins_path = sys.argv[2]\n",
    "output_path = sys.argv[3]\n",
    "</code>\n",
    "<code>\n",
    "phi_df = spark. \\\n",
    "    read. \\\n",
    "    format(\"csv\"). \\\n",
    "    option(\"inferSchema\",\"true\"). \\\n",
    "    option(\"header\", \"true\"). \\\n",
    "    load(phi_path)#path passed as argument\n",
    "</code>\n",
    "<code>\n",
    "jh_df = spark. \\\n",
    "    read. \\\n",
    "    format(\"csv\"). \\\n",
    "    schema(\"jh_Date date, jh_Country string, jh_Province string, jh_Lat double, jh_Long double, \\\n",
    "    jh_Confirmed integer, jh_Recovered integer, jh_Deaths integer\"). \\\n",
    "    option(\"header\", \"true\"). \\\n",
    "    load(john_hopkins_path)\n",
    "</code>\n",
    "<code>\n",
    "df_Final.write. \\\n",
    "    format(\"csv\"). \\\n",
    "    mode(\"overwrite\"). \\\n",
    "    save(output_path)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to Create a new AWS Data Pipeline service to automate creation of AWS EMR jobs\n",
    "* Under Analytics section chose Data Pipeline service. Click on Create new pipeline.\n",
    "* Give a name to your pipeline. I have given it the name My-Spark-Data-Pipeline.\n",
    "* Under Build using a template chose Run Job on Elastic MapReduce cluster.\n",
    "  * Under EMR step(s) sections place the following code:\n",
    "  <code>\n",
    "  command-runner.jar,spark-submit,--deploy-mode,cluster,s3://covid-19-tracker-2020/python/Spark-Job.py,s3://covid-19-tracker-2020/data/phi_CA.csv,s3://covid-19-tracker-2020/data/john_hopkins.csv,s3://covid-19-tracker-2020/output/tracker_output\n",
    "  </code>\n",
    "  command-runner.jar – This application package can execute the following list of functions including more\n",
    "* <b>hadoop-streaming</b>\n",
    "    * Submit a Hadoop streaming program. In the console and some SDKs, this is a streaming step. \n",
    "* <b>hive-script</b>\n",
    "    * Run a Hive script. In the console and SDKs, this is a Hive step.\n",
    "* <b>spark-submit</b><<< In place of manually specifying spark-submit in bash, we are passing                       command-runner.jar as EMR step\n",
    "    * Run a Spark application. In the console, this is a Spark step.\n",
    "* Choose core instance and master instance type as m5a.xlarge and EMR label 5.31.0.\n",
    "* Under Bootstrap action(s) pass the bootstrap script path\n",
    "<code>\n",
    "s3://covid-19-tracker-2020/bootstrap/bootstrap.sh\n",
    "</code>\n",
    "* Specify a Schedule for running your pipeline jobs:\n",
    "    I have specified configuration to run the job every 1 day starting on pipeline activation\n",
    "    <img src=\"Schedule.png\" width=\"700\">\n",
    "* Optionally Enable logging and choose the default Security Roles. \n",
    "* Click on Edit in Architect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline definition updates\n",
    "* Inside your JSON configuration insert a new key pair “applications”: “spark”\n",
    "<code>\n",
    "{\n",
    "    \"taskInstanceType\": \"#{myTaskInstanceType}\",\n",
    "    \"coreInstanceCount\": \"#{myCoreInstanceCount}\",\n",
    "    \"masterInstanceType\": \"#{myMasterInstanceType}\",\n",
    "    \"releaseLabel\": \"#{myEMRReleaseLabel}\",\n",
    "    \"type\": \"EmrCluster\",\n",
    "    \"terminateAfter\": \"50 Minutes\",\n",
    "    \"bootstrapAction\": \"#{myBootstrapAction}\",\n",
    "    \"taskInstanceCount\": \"#{myTaskInstanceCount}\",s\n",
    "    \"name\": \"EmrClusterObj\",\n",
    "    \"coreInstanceType\": \"#{myCoreInstanceType}\",\n",
    "    \"keyPair\": \"#{myEC2KeyPair}\",\n",
    "    \"id\": \"EmrClusterObj\",\n",
    "    \"applications\": \"spark\"# AWS Pipeline by default does not install spark libraries on cluster bootup as is the case with AWS EMR, we have to manually specidy the application\n",
    "}\n",
    "</code>\n",
    "* Now click on Activate to activate the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the pipeline progress on AWS EMR\n",
    "* Under Steps sections we note that it has executed two steps\n",
    "<code><img src=\"Steps.png\" width=700></code>\n",
    "    * Install <b>TaskRunner</b> – In this step our pipeline object EmrClusterObj unpacks various HADOOP and JDBC libraries on our cluster including\n",
    "    <code>\n",
    "    ZIP_FILE = http://datapipeline-us-east-1.s3.amazonaws.com/us-east-1/software/latest/TaskRunner/TaskRunner-1.0.zip\n",
    "MYSQL_FILE = http://datapipeline-us-east-1.s3.amazonaws.com/us-east-1/software/latest/TaskRunner/mysql-connector-java-bin.jar\n",
    "HIVE_CSV_SERDE_FILE = http://datapipeline-us-east-1.s3.amazonaws.com/us-east-1/software/latest/TaskRunner/csv-serde.jar\n",
    "HADOOP_CLASSPATH:/mnt/taskRunner/common/mysql-connector-java-bin.jar:/etc/hadoop/hive/lib/hive-exec.jar\n",
    "    </code>\n",
    "    * <b>EmrActivity</b> – This step executes out main Spark-Job.py script using the command-runner.jar application which runs the spark-submit command.\n",
    "    The first occurrence of our Data Pipeline has finished successfully\n",
    "    <img src=\"EmrActivity.png\" width=\"700\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
