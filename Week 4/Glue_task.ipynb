{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "essential-decline",
   "metadata": {},
   "source": [
    "#### Configure Crawler\n",
    "* Under Analytics choose Glue Service.\n",
    "* Under Crawler section, Click on Add Crawler.\n",
    "    * Crawler: A crawler connects to a data store, progresses through a prioritized list of classifiers to determine the schema for your data, and then creates metadata tables in your data catalog.\n",
    "* Give a name to your crawler, I have given it a name My-Glue-Catalog.\n",
    "* Choose crawler source type, I have chosen Data stores.\n",
    "    * Crawler types: \n",
    "        * Data stores – Amazon S3, JDBC, DynamoDB, MongoDB etc.\n",
    "        * Existing catalog tables\n",
    "<img src=\"Data_store.png\" width=\"700\">\n",
    "* Choose a data store. In my case I have chosen S3.\n",
    "* Choose an existing IAM Role. I have created an IAM Role My-Glue-Access-Role with the following policies:\n",
    "    * AmazonS3FullAccess\n",
    "    * AWSGlueServiceRole\n",
    "<img src=\"IAM_Role.png\" width=\"700\">\n",
    "* Choose a schedule to run this crawler (Daily, Hourly, Weekly, Monthly). I have chosen the default On-demand.\n",
    "* Choose crawler’s output directory. This is the place where crawler will store the S3 data source object in the form of input catalog table for our python script.\n",
    "<img src=\"Output.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-treasury",
   "metadata": {},
   "source": [
    "#### Configure Jobs\n",
    "A job is a business logic to perform ETL work\n",
    "* Choose a name for your job. I have chosen the name My-Spark-Job.\n",
    "* Attach your IAM role to this job.\n",
    "* Select the type of Job (Spark, Spark-Streaming or Python shell). I have selected Spark for batch job.\n",
    "* Select the Glue version. I have selected Glue 2.0 since it has support for Python 3.\n",
    "<img src=\"Job.png\" width=\"700\">\n",
    "* Chose the S3 path where the script will be saved.\n",
    "* Under job parameters, chose Worker type as Standard and Number of workers to a minimum of 2 since we are dealing with a small dataset.\n",
    "    * Worker types supported: \n",
    "        * Standard\n",
    "        * G.1X – for memory-intensive jobs\n",
    "        * G.2X – for Machine Learning applications\n",
    "* Chose your data source, in our case it is the data catalog table that we created.\n",
    "<img src=\"Data_Source.png\" width=\"700\">\n",
    "* Select transformation type to Change Schema and Click Next.\n",
    "* Under data target, choose data source as Amazon S3 and define a path. This will create an output catalog table and store the table in our data store in ‘CSV’ format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-fiber",
   "metadata": {},
   "source": [
    "#### Proposed script and updates (My-Spark-Job.py)\n",
    "* Script updates\n",
    "<code>\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext# Wraps the Apache SparkSQL SQLContext object\n",
    "</code>\n",
    "<img src=\"Script.png\" width=\"700\">\n",
    "* Using glueContext object we create a DynamicFrame (datasource0)\n",
    "<code>\n",
    "#DynamicFrame for the public health infobase catalog\n",
    "## @type: DataSource\n",
    "## @args: [database = \"input_database\", table_name = \"phi_ca_csv\", transformation_ctx = \"datasource0\"]\n",
    "## @return: datasource0\n",
    "## @inputs: []\n",
    "datasource0 = glueContext.create_dynamic_frame.from_catalog(database=\"input_database\", table_name=\"phi_ca_csv”, transformation_ctx=\"datasource0\")# Returns a DynamicFrame\n",
    "</code>\n",
    "DynamicFrames:\n",
    "* In DynamicFrames, Records are represented in a flexible self-describing way that preserves information about schema inconsistencies in the data.\n",
    "* DynamicFrames are also integrated with the AWS Glue Data Catalog, so creating frames from tables is a simple operation.\n",
    "<code>\n",
    "#DynamicFrame for the john hopkins catalog\n",
    "datasource1 = glueContext.create_dynamic_frame.from_catalog(database=\"input_database\", table_name=\"john_hopkins_csv\", transformation_ctx=\"datasource1\")\n",
    "</code>\n",
    "* Convert your DynamicFrame to an Apache Spark DataFrame by converting DynamicRecords into DataFrame fields.\n",
    "<code>\n",
    "phi_df = datasource0.toDF()# Returns DataFrame\n",
    "jh_df = datasource1.toDF()\n",
    "</code>\n",
    "* After performing our dataframe operations, we write our dataframe back into a DynamicFrame.\n",
    "<code>\n",
    "df_Final_Dynamic = DynamicFrame.fromDF(df_Final, glueContext, 'df_Final_Dynamic') #df_Final is the name of our dataframe, glueContext is a GlueContext object and ‘df_Final_Dynamic’ is our final DynamicFrame\n",
    "</code>\n",
    "* Writing our final DynamicFrame into our output sink.\n",
    "<code>\n",
    "# df1 = ResolveChoice.apply(df_Final_Dynamic, choice = \"make_cols\")\n",
    "## @type: DataSink\n",
    "## @args: [connection_type = \"s3\", connection_options = {\"path\": \"s3://covid-19-tracker-2020/glue/output_data\"}, format = \"csv\", transformation_ctx = \"datasink2\"]\n",
    "## @return: datasink2\n",
    "## @inputs: [frame = df_Final_Dynamic]\n",
    "datasink2 = glueContext.write_dynamic_frame.from_options(frame=df_Final_Dynamic, connection_type=\"s3\", connection_options={\"path\": \"s3://covid-19-tracker-2020/glue/output_data\"}, format=\"csv\", transformation_ctx=\"datasink2\")\n",
    "job.commit()\n",
    "</code>\n",
    "DynamicFrameWriter class takes three parameters:\n",
    "* frame – The DynamicFrame to write\n",
    "* connection_type - The connection type. Valid values include s3, mysql, postgresql, redshift, sqlserver, and oracle.\n",
    "* Connection_options - Connection options, such as path and database table (optional). For a connection_type of S3, an Amazon S3 path is defined.\n",
    "<code>\n",
    "connection_options={\"path\": \"s3://covid-19-tracker-2020/glue/output_data\"}\n",
    "</code>\n",
    "* Output csv data is stored in s3://covid-19-tracker/glue/output-data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
