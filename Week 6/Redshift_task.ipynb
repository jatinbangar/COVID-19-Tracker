{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "statewide-donor",
   "metadata": {},
   "source": [
    "#### Provisioning an Amazon Redshift Cluster\n",
    "* Under Analytics, Choose Amazon Redshift. Click on Create Cluster.\n",
    "* Under Cluster Identifier give a Name of your cluster. I have given the name My-Cluster.\n",
    "* Choose free trial. By default, free trial cluster selects dc2.large | 1 node cluster with 2vCPU.\n",
    "<img src=\"Cluster_Configuration.png\" width=\"700\">\n",
    "* Under database configurations, choose database name as dev and database port as 5439.\n",
    "* Also choose a username and password for your database.\n",
    "<img src=\"Database_Configuration.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-climb",
   "metadata": {},
   "source": [
    "#### Query Editor\n",
    "* Under Redshift service, go to Query Editor.\n",
    "* Connect to database using database credentials. Click on Connect to Database.\n",
    "<img src=\"Connect_Database.png\" width=\"700\">\n",
    "* Run this query to Create a new table in Amazon Redshift\n",
    "<code>\n",
    "CREATE TABLE public.phi_table(\n",
    "  phi_Date date NOT NULL,\n",
    "  phi_Province varchar NOT NULL,\n",
    "  phi_Confirmed integer,\n",
    "  phi_Deaths integer\n",
    ")\n",
    "</code>\n",
    "<img src=\"Query_Editor.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-hampshire",
   "metadata": {},
   "source": [
    "#### Using AWS Glue Service to Import data into Redshift\n",
    "There are different methods to Import our data from S3 bucket into a Redshift table, some of these are\n",
    "1. COPY method - The COPY query loads data in parallel from Amazon S3, Amazon EMR, Amazon DynamoDB, or multiple data sources on remote hosts. COPY loads large amounts of data much more efficiently than using INSERT statements, and stores the data more effectively as well.\n",
    "\n",
    "        copy <table_name> from 's3://<bucket_name>/<manifest_file>'\n",
    "        authorization\n",
    "        manifest;\n",
    "        \n",
    "2. Create AWS Glue Job\n",
    "\n",
    "* Go to AWS Glue service and Create a Classifier.\n",
    "    * Choose Classifier type as CSV and Column delimiter for CSV. Give a Name to your Classifier and Click on Create.\n",
    "<img src=\"Add_Classifier.png\" width=\"700\">\n",
    "* Create a Crawler to connect to a data source which in our case is S3 bucket.\n",
    "    * Go to Crawler section and Click on Add Crawler. Give a name to your crawler and select our Classifier.\n",
    "    <img src=\"Add_Crawler.png\" width=\"700\">\n",
    "    * Choose Crawler source type as data store and choose data store as S3 and type in path to your S3 bucket.\n",
    "    <img src=\"Add_Data_Store.png\" width=\"700\">\n",
    "    * Choose IAM Role to allow crawler to access S3 data stores.\n",
    "    * Click on Add database to create a new database catalog to contain all the tables created by this crawler.\n",
    "    <img src=\"Crawler_Output.png\" width=\"700\">\n",
    "    * Click on Finish to create a CSV Crawler. Click on Run Crawler to run your crawler to import metadata from your source files into tables.\n",
    "* Setting Up Redshift Connection\n",
    "    * Under AWS Glue, go to Connections and choose Add Connection.\n",
    "    * Give a Connection Name and Choose Connection type as Amazon Redshift.\n",
    "    <img src=\"Add_Connection.png\" width=\"700\">\n",
    "    * Choose your Redshift Cluster and enter your Redshift Database username and password and click on Finish.\n",
    "    * To successfully test this connection to your Redshift Cluster from AWS Glue, we need to configure VPC Endpoint for AWS Glue service to connect to S3.\n",
    "* Set Up VPC Gateway Endpoint Connection from AWS Glue to S3.\n",
    "    * Go to Amazon VPC service and Click on Endpoints.\n",
    "    * Click on Create Endpoint. Under Service category choose AWS services and under Service Name choose com.amazonaws.us-east-1.s3.\n",
    "    <img src=\"Create_Endpoint.png\" width=\"700\">\n",
    "    * Choose your default VPC and Router Table ID and Click Create Endpoint.\n",
    "* Create a new Crawler to Connect to our Redshift database and identify the schema of our redshift tables.\n",
    "    * Under AWS Glue, Go to Add Crawler and give it a name.\n",
    "    * Choose Crawler source type as data stores.\n",
    "    * Under data store choose JDBC and Under Connection choose the connection RedshiftCluster that we already defined.\n",
    "    * Choose the input path in the format database/schema/table and Click Next.\n",
    "    <img src=\"Edit_Crawler.png\" width=\"700\">\n",
    "    * Choose our existing IAM Role and Click Next.\n",
    "    * Choose Output database catalog Name and Click Next. Review and Click Finish.\n",
    "    <img src=\"Output_Database.png\" width=\"700\">\n",
    "* Create a Job to transfer S3 data to Redshift tables\n",
    "    * Under Glue Go to Jobs. Click on Add Job.\n",
    "    * Give a Name to your Job and Choose the existing Glue Role. Click on Next.\n",
    "    <img src=\"Add_Job.png\" width=\"700\">\n",
    "    * Choose your data source as the S3 catalog and Click Next.\n",
    "    <img src=\"Data_Source.png\" width=\"700\">\n",
    "    * Choose a data target as the Redshift catalog and Click Next.\n",
    "    <img src=\"Data_Target.png\" width=\"700\">\n",
    "    * Configure mapping from source schema to output schema\n",
    "    <img src=\"Output_Schema.png\" width=\"700\">\n",
    "    * Click on Save Job and Edit Script.\n",
    "    * Click on Run Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-mechanism",
   "metadata": {},
   "source": [
    "#### Performing Analysis Using Redshift Query Editor\n",
    "* Run the below command to confirm that the tables populated well\n",
    "<code>\n",
    "SELECT\n",
    "  *\n",
    "FROM \n",
    "  public.jh_table \n",
    "WHERE\n",
    "  jh_province = 'Ontario'\n",
    "</code>\n",
    "<img src=\"Query_1.png\" width=\"700\">\n",
    "Output\n",
    "<code>\n",
    "jh_date,jh_country,jh_province,jh_lat,jh_long,jh_confirmed,jh_recovered,jh_deaths\n",
    "2020-01-22,Canada,Ontario,51.2538,-85.3232,0,,0\n",
    "2020-01-24,Canada,Ontario,51.2538,-85.3232,0,,0\n",
    "2020-01-26,Canada,Ontario,51.2538,-85.3232,1,,0\n",
    "2020-01-28,Canada,Ontario,51.2538,-85.3232,1,,0\n",
    "2020-01-30,Canada,Ontario,51.2538,-85.3232,1,,0\n",
    "2020-02-01,Canada,Ontario,51.2538,-85.3232,3,,0\n",
    "2020-02-03,Canada,Ontario,51.2538,-85.3232,3,,0\n",
    "2020-02-05,Canada,Ontario,51.2538,-85.3232,3,,0\n",
    "2020-02-07,Canada,Ontario,51.2538,-85.3232,3,,0\n",
    "2020-02-09,Canada,Ontario,51.2538,-85.3232,3,,0\n",
    "</code>\n",
    "* Export our final table into our S3 bucket in a different file format than the source data\n",
    "UNLOAD method \n",
    "Unloads the result of a query to one or more text or Apache Parquet files on Amazon S3, using Amazon S3 server-side encryption (SSE-S3).\n",
    "\n",
    "        UNLOAD ('select-statement')\n",
    "        TO 's3://object-path/name-prefix'\n",
    "        authorization\n",
    "        [ option [ ... ] ]\n",
    "<code>\n",
    "UNLOAD ('SELECT * FROM final_table')\n",
    "TO 's3://covid-19-tracker-2020/redshift/output/'\n",
    "IAM_ROLE 'arn:aws:iam::234610110457:role/My-Redshift-IAM-Role'\n",
    "FORMAT PARQUET# Parquet format\n",
    "</code>\n",
    "<img src=\"Unload.png\" width=\"700\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
