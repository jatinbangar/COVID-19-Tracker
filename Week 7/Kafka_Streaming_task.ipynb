{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "scenic-hygiene",
   "metadata": {},
   "source": [
    "#### Spark Streaming\n",
    "* Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.\n",
    "<img src=\"Spark_Streaming.png\" width=\"700\">\n",
    "* There are four types of Streaming Data Sources\n",
    "    * Socket Source\n",
    "    * Rate Source\n",
    "    * File Source\n",
    "    * Kafka Source\n",
    "* Kafka Data Source\n",
    "    * Kafka Data Source is the streaming data source for Apache Kafka in Spark Structured Streaming.\n",
    "    * Kafka Data Source provides a streaming source and a streaming sink for micro-batch and continuous stream processing\n",
    "<img src=\"Kafka_Data_Source.png\" width=\"700\">\n",
    "* Micro -Batch Stream Processing vs Continuous Stream Processing\n",
    "    * Kafka Data Source supports Micro-Batch Stream Processing (i.e. Trigger.Once and Trigger.ProcessingTime triggers) via KafkaMicroBatchReader.\n",
    "    * Kafka Data Source supports Continuous Stream Processing (i.e. Trigger.Continuous trigger) via KafkaContinuousReader.\n",
    "<img src=\"Structured_Streaming.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-surgery",
   "metadata": {},
   "source": [
    "#### Create EMR Cluster for Running our Kafka Streaming Application\n",
    "* Create Cluster\n",
    "    * Go to AWS Services > EMR > Create Cluster.\n",
    "    * Go to advanced options .\n",
    "    * Select the release emr-6.1.0 with applications Hadoop 3.2.1, Hive 3.1.2 and Spark 3.0.0.\n",
    "    <img src=\"Create_Cluster.png\" width=\"700\">\n",
    "    * Choose 1 master and 2 core nodes with Instance Type m5a.xlarge.\n",
    "    <img src=\"Cluster_Nodes.png\" width=\"700\">\n",
    "    * Under Security Options, Select your EC2-Key-Pair to be able to SSH into your cluster and click Create Cluster.\n",
    "    <img src=\"Advanced_Options.png\" width=\"700\">\n",
    "    * Under Security Group Settings for master node configure SSH from your IP\n",
    "    <img src=\"Security_Group.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-world",
   "metadata": {},
   "source": [
    "#### Deploy the Spark Application as per the deployment section of \"Structured Streaming + Kafka Integration Guide”\n",
    "https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-grass",
   "metadata": {},
   "source": [
    "#### Kafka Setup \n",
    "* Download Kafka\n",
    "    * Go to https://kafka.apache.org/downloads\n",
    "    * Under 2.4.1, Binary downloads Choose Scala 2.12 \n",
    "    * Go to shell and type\n",
    "<code>\n",
    "wget https://archive.apache.org/dist/kafka/0.10.2.2/kafka_2.12-0.10.2.2.tgz\n",
    "tar xzf kafka_2.12-0.10.2.2.tgz\n",
    "mv kafka_2.12-0.10.2.2 kafka\n",
    "</code>\n",
    "<img src=\"Download_Kafka.png\" width=\"700\">\n",
    "* Running Zookeeper\n",
    "<code>\n",
    "bin/zookeeper-server-start.sh config/zookeeper.properties\n",
    "</code>\n",
    "* Running Kafka broker\n",
    "    * Duplicate Session and run the below command\n",
    "    <code>\n",
    "    bin/kafka-server-start.sh config/server.properties\n",
    "    </code>\n",
    "    <img src=\"Kafka_Broker.png\" width=\"700\">\n",
    "* Create producer topic\n",
    "<code>\n",
    "bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic topic-a\n",
    "</code>\n",
    "<img src=\"Producer_Topic.png\" width=\"700\">\n",
    "* Create consumer topic\n",
    "<code>\n",
    "bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic topic-b\n",
    "</code>\n",
    "Check topic creation\n",
    "bin/kafka-topics.sh --list --zookeeper ip-172-31-34-198:2181\n",
    "<img src=\"Topic_Creation.png\" width=\"700\">\n",
    "* Launch producer job\n",
    "<code>\n",
    "bin/kafka-console-producer.sh --broker-list localhost:9092 --topic topic-a\n",
    "</code>\n",
    "<img src=\"Producer_Job.png\" width=\"700\">\n",
    "* Launch consumer job\n",
    "<code>\n",
    "bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic-b --from-beginning\n",
    "</code>\n",
    "<img src=\"Consumer_Job.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-civilization",
   "metadata": {},
   "source": [
    "#### PySpark Setup\n",
    "* Install PySpark\n",
    "<code>\n",
    "pip install pyspark –-user\n",
    "</code>\n",
    "* Getting additional jar files\n",
    "spark-streaming-kafka-0-10-assembly_2.12-3.0.0.jar\n",
    "<code>\n",
    "wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10-assembly_2.12/3.0.0/spark-streaming-kafka-0-10-assembly_2.12-3.0.0.jar\n",
    "</code>\n",
    "spark-sql-kafka-0-10_2.12-3.0.0.jar\n",
    "<code>\n",
    "wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.0.0/spark-sql-kafka-0-10_2.12-3.0.0.jar\n",
    "</code>\n",
    "commons-pool2-2.8.0.jar\n",
    "<code>\n",
    "wget https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.8.0/commons-pool2-2.8.0.jar\n",
    "</code>\n",
    "kafka-clients-0.10.2.2.jar\n",
    "<code>\n",
    "wget https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/0.10.2.2/kafka-clients-0.10.2.2.jar\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-skiing",
   "metadata": {},
   "source": [
    "#### Running PySpark Job\n",
    "* Run PySpark Kafka Streaming Code \n",
    "<code>\n",
    "spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 /home/hadoop/capstone/Kafka-Sink-Job.py#We provide the maven dependency in packages groupId:artifactId:version \n",
    "</code>\n",
    "\n",
    "    * Load method returns a Streaming DataFrame\n",
    "    <code>\n",
    "    kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"topic-a\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load() \n",
    "    <code>\n",
    "    * Assigning a schema to a json string can be achieved using from_json() function\n",
    "    <code>\n",
    "    value_df = kafka_df.select(from_json(col(\"value\").cast(\"string\"),schema).alias(\"value\"))\n",
    "    </code>\n",
    "    * writeStream method gives us the DataStreamWriter, start() method is like an action which starts a background Spark Job\n",
    "    <code>\n",
    "    notification_writer_query = kafka_target_df \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"Notification Writer\") \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"topic-b\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"chk-point-dir\") \\\n",
    "    .start()\n",
    "    </code>\n",
    "    * Start method is a non-blocking method that starts a background job and returns, so we must wait for the background job to complete\n",
    "    <code>\n",
    "    notification_writer_query.awaitTermination()\n",
    "    </code>\n",
    "    \n",
    "* Input Records\n",
    "Our Input Records consist of the total Confirmed Cases, Total Recovered, Total Deaths and Todays Confirmed cases of COVID-19 for the date 30-09-2020\n",
    "<img src=\"Input_Records.png\" width=\"700\">\n",
    "\n",
    "* Pushing first record to the producer node\n",
    "    * {\"Date\":\"30-09-2020\",\"Province\":\"Alberta\",\"Confirmed\":18062,\"Recovered\":16213,\"Deaths\":267,\"NumberToday\":153}\n",
    "    <img src=\"Producer_Record_1.png\" width=\"700\">\n",
    "    * The record is processed in the micro-batch and pushed to the consumer node\n",
    "    <img src=\"Consumer_Record_1.png\" width=\"700\">\n",
    "    * Pushing second record to the producer node\n",
    "    <img src=\"Producer_Record_2.png\" width=\"700\">\n",
    "    * As soon as new input is received the Spark background process triggers a new micro-batch which reads the new record from producer, applies transformation and pushes the output dataframe to the consumer node\n",
    "    <img src=\"Consumer_Record_2.png\" width=\"700\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
